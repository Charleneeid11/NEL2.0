{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90d9220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, Normalizer\n",
    "\n",
    "class DataWrapper:\n",
    "    def __init__(self, x, y, test_size):\n",
    "        self.x, self.y = x, y\n",
    "        self.test_size = test_size\n",
    "        scaler = StandardScaler()\n",
    "        normalizer = Normalizer()\n",
    "        \n",
    "        if isinstance(self.x, pd.DataFrame) or isinstance(self.x, pd.Series):\n",
    "            self.x = self.x.to_numpy()\n",
    "        if isinstance(self.y, pd.DataFrame) or isinstance(self.y, pd.Series):\n",
    "            self.y = self.y.to_numpy()\n",
    "\n",
    "        if test_size == 0:\n",
    "            self.x_train = self.x\n",
    "            self.y_train = self.y\n",
    "\n",
    "            self.x_train = self.x_train.astype('float32')\n",
    "            self.x_train = self.x_train.reshape(-1, 1)\n",
    "            self.x_train = normalizer.fit_transform(self.x_train)\n",
    "\n",
    "            self.y_train = self.y_train.reshape(-1, 1)\n",
    "                \n",
    "            return\n",
    "    \n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.x, self.y, test_size=self.test_size, shuffle=True)\n",
    "        \n",
    "        self.x_train = self.x_train.astype('float32')\n",
    "        self.x_test = self.x_test.astype('float32')\n",
    "\n",
    "        self.x_train = self.x_train.reshape(-1, 1)\n",
    "        self.x_test = self.x_test.reshape(-1, 1)\n",
    "\n",
    "        self.x_train = normalizer.fit_transform(self.x_train)\n",
    "        self.x_test = normalizer.transform(self.x_test)\n",
    "\n",
    "\n",
    "        self.y_train = self.y_train.reshape(-1, 1)\n",
    "        self.y_test = self.y_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"Data Preview:\n",
    "        x_train shape: {self.x_train.shape}\n",
    "        y_train shape: {self.y_train.shape}\n",
    "        First x_train sample: {self.x_train[0]}\n",
    "        First y_train sample: {self.y_train[0]}\n",
    "        -----------------------------\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d265d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "class NeuralNetworkParams:\n",
    "    def __init__(self, number_of_hidden_layers, number_of_neurons_in_hidden_layers, hidden_layers_activation_function, output_layer_activation_function, learning_rate):\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_neurons_in_hidden_layers = number_of_neurons_in_hidden_layers\n",
    "        self.hidden_layers_activation_function = hidden_layers_activation_function\n",
    "        self.output_layer_activation_function = output_layer_activation_function\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, params: NeuralNetworkParams, data: DataWrapper):\n",
    "        self.params = params\n",
    "        self.data = data\n",
    "        self.model = self.generate_model(params, data)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "        \n",
    "    def generate_model(self, params: NeuralNetworkParams, data: DataWrapper):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(\n",
    "            tf.keras.layers.InputLayer(input_shape=[data.x_train.shape[1]]))\n",
    "\n",
    "        for i in range(params.number_of_hidden_layers):\n",
    "            model.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    params.number_of_neurons_in_hidden_layers,\n",
    "                    activation=params.hidden_layers_activation_function,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=params.learning_rate),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=[\"accuracy\", tf.keras.metrics.Recall()],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, model_index, epochs=100, verbose=0, draw_accuracy=True):\n",
    "\n",
    "        print(\"\\n-------------------\\nTraining model: \", model_index + 1, \"\\n-------------------\\n\")\n",
    "\n",
    "        print(\"Data: \", self.data)\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.data.x_train,\n",
    "            self.data.y_train,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            validation_data=(self.data.x_val, self.data.y_val)\n",
    "        )\n",
    "        if draw_accuracy:\n",
    "            utils.plot_accuracy_loss(history, model_index, is_validation_data=True, save_to_folder=\"./logs/models_accuracy_images\")\n",
    "\n",
    "        print(\"\\nTraining accuracy: \", history.history[\"accuracy\"][-1], \"\\n\")\n",
    "        print(\"Training loss: \", history.history[\"loss\"][-1], \"\\n-------------------\\n\")\n",
    "\n",
    "        print(\"Validation accuracy: \", history.history[\"val_accuracy\"][-1], \"\\n\")\n",
    "        print(\"Validation loss: \", history.history[\"val_loss\"][-1], \"\\n-------------------\\n\")\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def test(self):\n",
    "        print(\"\\nEvaluating on test data:...\\n\")\n",
    "        results = self.model.evaluate(\n",
    "        self.data.x_test,\n",
    "        self.data.y_test,\n",
    "        verbose=2)\n",
    "        print(\"Test loss:\", results[0], \"\\n-------------------\\n\")\n",
    "        print(\"Test accuracy:\", results[1], \"\\n-------------------\\n\")\n",
    "        if len(results) > 2:\n",
    "            print(\"Test additional metrics:\", results[2:], \"\\n-------------------\\n\")\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def train_and_test(self, model_index):\n",
    "        self.train(model_index, epochs=100, verbose=0, draw_accuracy=True)\n",
    "        self.test()\n",
    "\n",
    "    def set_weights(self, new_weights):\n",
    "        self.model.set_weights(new_weights)\n",
    "\n",
    "    def evaluate(self):\n",
    "        return self.model.evaluate(self.data.x_test, self.data.y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040f141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetworkParamsGenerator:\n",
    "    def __init__(self):    \n",
    "        self.number_of_hidden_layers = [1, 4]\n",
    "        self.number_of_neurons_in_hidden_layers = [10, 100]\n",
    "        self.hidden_layers_activation_function = [1, 3]\n",
    "        self.output_layer_activation_function = [1, 2]\n",
    "        self.learning_rate = [0.00001, 0.1]\n",
    "        \n",
    "    def get_random_params(self):\n",
    "        return NeuralNetworkParams(\n",
    "            number_of_hidden_layers = np.random.randint(self.number_of_hidden_layers[0], self.number_of_hidden_layers[1]),\n",
    "            number_of_neurons_in_hidden_layers = np.random.randint(self.number_of_neurons_in_hidden_layers[0], self.number_of_neurons_in_hidden_layers[1]),\n",
    "            hidden_layers_activation_function = np.random.choice(self.hidden_layers_activation_function),\n",
    "            output_layer_activation_function = np.random.choice(self.output_layer_activation_function),\n",
    "            learning_rate = np.random.uniform(self.learning_rate[0], self.learning_rate[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bc0817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# multi-threading and multi-processing\n",
    "import threading\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self, data: DataWrapper, number_of_models, epoch: int, edge_prob: float, sim_type: SimType, cent_type : CentType):\n",
    "        if not 50 <= epoch <= 200:\n",
    "            raise ValueError(\"Epoch value must be between 50 and 200 inclusive.\")\n",
    "        if not 0.2 <= edge_prob <= 1:\n",
    "            raise ValueError(\"Edge_prob value must be between 0.2 and 1 inclusive.\")\n",
    "        self.data = data\n",
    "        self.neural_network_params_generator = NeuralNetworkParamsGenerator()\n",
    "        self.number_of_models = number_of_models\n",
    "        self.epoch = epoch\n",
    "        self.edge_prob = edge_prob\n",
    "        self.sim_type = sim_type\n",
    "        self.cent_type = cent_type\n",
    "        self.models_data = []\n",
    "        self.models = []\n",
    "        self.allocate_models_data()\n",
    "        self.generate_models()\n",
    "\n",
    "    def allocate_models_data(self):\n",
    "        for i in range(self.number_of_models):\n",
    "            self.models_data.append(DataWrapper(self.data, self.data, test_size=0.2))\n",
    "        \n",
    "    def generate_models(self):\n",
    "        for i in range(self.number_of_models):\n",
    "            self.models.append(NeuralNetwork(self.neural_network_params_generator.get_random_params(), self.models_data[i]))\n",
    "\n",
    "    def train_ind(self, model_index):\n",
    "        # multi-processing\n",
    "        # run each model in a separate process in parallel\n",
    "        processes = []\n",
    "        for i in range(self.number_of_models):\n",
    "            processes.append(multiprocessing.Process(target=self.models[i].train_and_test, args=(model_index)))\n",
    "            processes[i].start()\n",
    "        for i in range(self.number_of_models):\n",
    "            processes[i].join()\n",
    "\n",
    "    def vectorize_model(self, i):\n",
    "        norm_hidden_layers=((self.models[i].number_of_hidden_layers)-1)/3\n",
    "        norm_neurons=((self.models[i].number_of_neurons_in_hidden_layers)-10)/90\n",
    "        norm_hlaf=((self.models[i].hidden_layers_activation_function)-1)/2\n",
    "        norm_olaf=((self.models[i].output_layer_activation_function)-1)\n",
    "        norm_lr=((self.models[i].learning_rate)-0.00001)/0.09999\n",
    "        normalized_params=[]\n",
    "        normalized_params.extend(norm_hidden_layers, norm_neurons, norm_hlaf, norm_olaf, norm_lr)\n",
    "        return normalized_params\n",
    "\n",
    "    def cosine_similarity(self, i, j):\n",
    "        v1=self.vectorize_model(i)\n",
    "        v2=self.vectorize_model(j)\n",
    "        similarity_matrix=cosine_similarity(v1, v2)\n",
    "        return similarity_matrix\n",
    "        \n",
    "    def evaluate_models(self):\n",
    "        loss, accuracy = [], []\n",
    "        \n",
    "        processes = []\n",
    "        for i in range(self.number_of_models):\n",
    "            processes.append(multiprocessing.Process(target=self.models[i].evaluate))\n",
    "            processes[i].start()\n",
    "        for i in range(self.number_of_models):\n",
    "            processes[i].join()\n",
    "            \n",
    "        \n",
    "        # get loss and accuracy from each model\n",
    "        for i in range(self.number_of_models):\n",
    "            loss.append(processes[i].loss)\n",
    "            accuracy.append(processes[i].accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ae6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class SimType(Enum):\n",
    "    COSINE = \"cosine\"\n",
    "    EUCLIDEAN = \"euclidean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68534c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class CentType(Enum):\n",
    "    DEGREE = \"degree\"\n",
    "    EIGENVECTOR = \"eigenvector\"\n",
    "    CLOSENESS = \"closeness\"\n",
    "    BETWEENNESS = \"betweenness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "203b94ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<__main__.DataWrapper object at 0x000002342C121E50>, dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     ensemble\u001b[38;5;241m.\u001b[39mtrain_ind(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, draw_accuracy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#ensemble.evaluate_models()\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m DataWrapper(x, y, \u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m Ensemble(data, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m ensemble\u001b[38;5;241m.\u001b[39mtrain_ind(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, draw_accuracy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[20], line 26\u001b[0m, in \u001b[0;36mEnsemble.__init__\u001b[1;34m(self, data, number_of_models, epoch, edge_prob, sim_type, cent_type)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallocate_models_data()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_models()\n",
      "Cell \u001b[1;32mIn[20], line 31\u001b[0m, in \u001b[0;36mEnsemble.allocate_models_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallocate_models_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_models):\n\u001b[1;32m---> 31\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_data\u001b[38;5;241m.\u001b[39mappend(DataWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m))\n",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m, in \u001b[0;36mDataWrapper.__init__\u001b[1;34m(self, x, y, test_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2614\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2619\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:455\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 455\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:406\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[0;32m    396\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    407\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:406\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[0;32m    396\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    407\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:347\u001b[0m, in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingleton array \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m cannot be considered a valid collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m x\n\u001b[0;32m    349\u001b[0m         )\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;66;03m# Check that shape is returning an integer or default to len\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;66;03m# Dask dataframes may not return numeric shape[0] value\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(<__main__.DataWrapper object at 0x000002342C121E50>, dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"C:/Users/charl/Desktop/Documents/LAU/Semesters/Grad/Courses Taken/Fall2023/Topics - Network Science/Final Project/car data.csv\")\n",
    "    x = df.iloc[:, 0]\n",
    "    y = df.iloc[:, 1]\n",
    "    data = DataWrapper(x, y, 0.2)\n",
    "    ensemble = Ensemble(data, 10, 100, 0.5, \"cosine\", \"degree\")\n",
    "    ensemble.train_ind(epochs=100, verbose=0, draw_accuracy=True)\n",
    "    #ensemble.evaluate_models()\n",
    "    \n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
